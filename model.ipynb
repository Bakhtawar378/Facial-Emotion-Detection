{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-09-08T17:01:27.512838Z",
     "iopub.status.busy": "2023-09-08T17:01:27.512187Z",
     "iopub.status.idle": "2023-09-08T17:02:53.514817Z",
     "shell.execute_reply": "2023-09-08T17:02:53.513367Z",
     "shell.execute_reply.started": "2023-09-08T17:01:27.512806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T04:57:13.291076Z",
     "iopub.status.busy": "2024-06-09T04:57:13.290805Z",
     "iopub.status.idle": "2024-06-09T04:57:24.897080Z",
     "shell.execute_reply": "2024-06-09T04:57:24.896199Z",
     "shell.execute_reply.started": "2024-06-09T04:57:13.291051Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dropout\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import BatchNormalization\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T04:57:52.525449Z",
     "iopub.status.busy": "2024-06-09T04:57:52.524823Z",
     "iopub.status.idle": "2024-06-09T04:57:52.530388Z",
     "shell.execute_reply": "2024-06-09T04:57:52.529453Z",
     "shell.execute_reply.started": "2024-06-09T04:57:52.525416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "img_rows, img_cols = 48, 48\n",
    "classes = ['angry', 'disgust','fear','happy','neutral','sad','surprise']\n",
    "# classes = ['disgust', 'fear', 'happy','pain', 'sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T04:57:54.954867Z",
     "iopub.status.busy": "2024-06-09T04:57:54.954455Z",
     "iopub.status.idle": "2024-06-09T04:57:55.196292Z",
     "shell.execute_reply": "2024-06-09T04:57:55.195499Z",
     "shell.execute_reply.started": "2024-06-09T04:57:54.954837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "\n",
    "def get_data(batch_size, target_size, train_path, test_path):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        validation_split=0.2,\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        preprocessing_function=lambda img: cv2.resize(img, (224,224), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "\n",
    "    )\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(\n",
    "        validation_split=0.2,\n",
    "        rescale=1./255\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        batch_size=batch_size,\n",
    "        target_size=target_size,\n",
    "        color_mode='rgb',\n",
    "        shuffle=True,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "        \n",
    "        \n",
    "    )  # Set as training data\n",
    "\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        test_path,\n",
    "        batch_size=batch_size,\n",
    "        target_size=target_size,\n",
    "        color_mode='rgb',\n",
    "        shuffle=False,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "        \n",
    "    )  # Set as validation data\n",
    "\n",
    "    return train_generator, train_generator.labels, validation_generator, validation_generator.labels\n",
    "# function to plot the confusion matrix\n",
    "def plot_confusion_matrix(classes, model, model_name):\n",
    "    batch_size=32\n",
    "    num_of_test_samples = 4941\n",
    "    target_names =classes\n",
    "    #Confution Matrix and Classification Report\n",
    "    Y_pred = model.predict(validation_generator, num_of_test_samples // batch_size)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    print('Confusion Matrix')\n",
    "    cm = confusion_matrix(validation_generator.classes, y_pred)\n",
    "    print(cm)\n",
    "    print('Classification Report')\n",
    "    print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "    # Normalise\n",
    "    cmn = cm.astype('float') / cm.sum(axis=1)\n",
    "    fig, ax = plt.subplots(figsize=(20,7))\n",
    "\n",
    "    sns.heatmap(cmn, center=0, annot=True, fmt='.2f', linewidths=1,  xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('cofusion_matrix of **' + f'( {model_name} )**')\n",
    "   \n",
    "    plt.show(block=False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#function to graph the accuracy and loss\n",
    "def plot_graph(history, model_name):\n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot([None] + history.history['accuracy'], 'o-')\n",
    "    ax.plot([None] + history.history['val_accuracy'], 'x-')\n",
    "    # Plot legend and use the best location automatically: loc = 0.\n",
    "    ax.legend(['Train acc', 'Validation acc'], loc = 0)\n",
    "    ax.set_title('Training/Validation acc per Epoch '+f'({model_name})')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "\n",
    " \n",
    "    plt.show()\n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot([None] + history.history['loss'], 'o-')\n",
    "    ax.plot([None] + history.history['val_loss'], 'x-')\n",
    "\n",
    "    # Plot legend and use the best location automatically: loc = 0.\n",
    "    ax.legend(['Train loss', \"Val loss\"], loc = 1)\n",
    "    ax.set_title('Training/Validation Loss per Epoch '+f'({model_name})')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "   \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T04:58:05.292082Z",
     "iopub.status.busy": "2024-06-09T04:58:05.291390Z",
     "iopub.status.idle": "2024-06-09T04:58:05.312297Z",
     "shell.execute_reply": "2024-06-09T04:58:05.311425Z",
     "shell.execute_reply.started": "2024-06-09T04:58:05.292035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D,GlobalMaxPooling2D,Dense,Reshape, Add, Activation,Permute,Lambda,Concatenate, Conv2D,multiply,Layer\n",
    "\n",
    "\n",
    "class CBAMBlock(keras.layers.Layer):\n",
    "    def __init__(self, ratio=64,name='cbamblock'):\n",
    "        super().__init__(name=name)\n",
    "        self.ratio = ratio\n",
    "\n",
    "    @tf.function\n",
    "    def cbam_block(self, cbam_feature):\n",
    "        cbam_feature = self.channel_attention(cbam_feature)\n",
    "        cbam_feature = self.spatial_attention(cbam_feature)\n",
    "        return cbam_feature\n",
    "\n",
    "    def channel_attention(self, input_feature):\n",
    "        channel_axis = 1 if tf.keras.backend.image_data_format() == \"channels_first\" else -1\n",
    "        channel = input_feature.shape[channel_axis]\n",
    "\n",
    "        shared_layer_one = Dense(\n",
    "            channel // self.ratio,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal',\n",
    "            use_bias=True,\n",
    "            bias_initializer='zeros',\n",
    "        )\n",
    "        shared_layer_two = Dense(\n",
    "            channel,\n",
    "            kernel_initializer='he_normal',\n",
    "            use_bias=True,\n",
    "            bias_initializer='zeros',\n",
    "        )\n",
    "\n",
    "        avg_pool = GlobalAveragePooling2D()(input_feature)\n",
    "        avg_pool = Reshape((1, 1, channel))(avg_pool)\n",
    "        assert avg_pool.shape[1:] == (1, 1, channel)\n",
    "        avg_pool = shared_layer_one(avg_pool)\n",
    "        assert avg_pool.shape[1:] == (1, 1, channel // self.ratio)\n",
    "        avg_pool = shared_layer_two(avg_pool)\n",
    "        assert avg_pool.shape[1:] == (1, 1, channel)\n",
    "\n",
    "        max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "        max_pool = Reshape((1, 1, channel))(max_pool)\n",
    "        assert max_pool.shape[1:] == (1, 1, channel)\n",
    "        max_pool = shared_layer_one(max_pool)\n",
    "        assert max_pool.shape[1:] == (1, 1, channel // self.ratio)\n",
    "        max_pool = shared_layer_two(max_pool)\n",
    "        assert max_pool.shape[1:] == (1, 1, channel)\n",
    "\n",
    "        cbam_feature = Add()([avg_pool, max_pool])\n",
    "        cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "        if tf.keras.backend.image_data_format() == \"channels_first\":\n",
    "            cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\n",
    "        return multiply([input_feature, cbam_feature])\n",
    "\n",
    "    def spatial_attention(self, input_feature):\n",
    "        kernel_size = 7\n",
    "\n",
    "        if tf.keras.backend.image_data_format() == \"channels_first\":\n",
    "            channel = input_feature.shape[1]\n",
    "            cbam_feature = Permute((2, 3, 1))(input_feature)\n",
    "        else:\n",
    "            channel = input_feature.shape[-1]\n",
    "            cbam_feature = input_feature\n",
    "\n",
    "        avg_pool = Lambda(lambda x: tf.reduce_mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "        assert avg_pool.shape[-1] == 1\n",
    "        max_pool = Lambda(lambda x: tf.reduce_max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "        assert max_pool.shape[-1] == 1\n",
    "        concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "        assert concat.shape[-1] == 2\n",
    "        cbam_feature = Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            activation='sigmoid',\n",
    "            kernel_initializer='he_normal',\n",
    "            use_bias=False,\n",
    "        )(concat)\n",
    "        assert cbam_feature.shape[-1] == 1\n",
    "\n",
    "        if tf.keras.backend.image_data_format() == \"channels_first\":\n",
    "            cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\n",
    "        return multiply([input_feature, cbam_feature])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T02:24:33.206915Z",
     "iopub.status.busy": "2023-10-27T02:24:33.206124Z",
     "iopub.status.idle": "2023-10-27T02:24:33.213463Z",
     "shell.execute_reply": "2023-10-27T02:24:33.212397Z",
     "shell.execute_reply.started": "2023-10-27T02:24:33.206883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply\n",
    "\n",
    "def se_block(inputs, ratio=4, pooling_type='avg'):\n",
    "    filters = inputs.shape[-1]\n",
    "    se_shape = (1, 1, filters)\n",
    "    \n",
    "    se = GlobalAveragePooling2D()(inputs)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='hard_sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    return multiply([inputs, se])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T02:24:40.006219Z",
     "iopub.status.busy": "2023-10-27T02:24:40.005629Z",
     "iopub.status.idle": "2023-10-27T02:24:40.012885Z",
     "shell.execute_reply": "2023-10-27T02:24:40.012072Z",
     "shell.execute_reply.started": "2023-10-27T02:24:40.006186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.saving.object_registration.CustomObjectScope at 0x7ae5170669b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.custom_object_scope({'SeBlock':se_block})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T04:58:14.412602Z",
     "iopub.status.busy": "2024-06-09T04:58:14.412202Z",
     "iopub.status.idle": "2024-06-09T04:58:16.256198Z",
     "shell.execute_reply": "2024-06-09T04:58:16.252808Z",
     "shell.execute_reply.started": "2024-06-09T04:58:14.412568Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 112, 112, 32)      896       \n",
      "                                                                 \n",
      " depthwise_conv2d (Depthwise  (None, 112, 112, 32)     320       \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 64)      2112      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 112, 112, 64)     256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " depthwise_conv2d_1 (Depthwi  (None, 56, 56, 64)       640       \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 56, 128)       8320      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 56, 56, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_2 (Depthwi  (None, 56, 56, 128)      1280      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 56, 56, 128)       16512     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 56, 56, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_3 (Depthwi  (None, 28, 28, 128)      1280      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 256)       33024     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 28, 28, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_4 (Depthwi  (None, 28, 28, 256)      2560      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 256)       65792     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 28, 28, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_5 (Depthwi  (None, 14, 14, 256)      2560      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 14, 14, 256)       65792     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 14, 14, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " cbamblock_1 (CBAMBlock)     (None, 14, 14, 256)       0         \n",
      "                                                                 \n",
      " depthwise_conv2d_6 (Depthwi  (None, 14, 14, 256)      2560      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 14, 14, 512)       131584    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 14, 14, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_7 (Depthwi  (None, 14, 14, 512)      5120      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 14, 14, 512)       262656    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 14, 14, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_8 (Depthwi  (None, 14, 14, 512)      5120      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 14, 14, 512)       262656    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 14, 14, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_9 (Depthwi  (None, 14, 14, 512)      5120      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 14, 14, 512)       262656    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 14, 14, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_10 (Depthw  (None, 14, 14, 512)      5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 14, 14, 512)       262656    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 14, 14, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " depthwise_conv2d_11 (Depthw  (None, 7, 7, 512)        5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 7, 7, 1024)        525312    \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 7, 7, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " depthwise_conv2d_12 (Depthw  (None, 7, 7, 1024)       10240     \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 7, 7, 1024)        1049600   \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 7, 7, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1024)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 7175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,076,167\n",
      "Trainable params: 4,064,775\n",
      "Non-trainable params: 11,392\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, GlobalAveragePooling2D, Dense, BatchNormalization\n",
    "\n",
    "from keras.models import Model\n",
    "# Input layer\n",
    "input_shape = (224, 224, 3)\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(input_layer) #conv/s2\n",
    "\n",
    "x = DepthwiseConv2D((3, 3), strides=(1, 1), padding='same', activation='relu')(x)#conv dw/s1\n",
    "x = Conv2D(64, (1, 1), strides=(1, 1), padding='same', activation=tf.nn.relu6)(x) #conv/s1\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = DepthwiseConv2D((3, 3), strides=(2, 2), padding='same', activation='relu')(x) #conv dw/s2\n",
    "x = Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation=tf.nn.relu6)(x) # conv/s1\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = DepthwiseConv2D((3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation=tf.nn.relu6)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = DepthwiseConv2D((3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "x = Conv2D(256, (1, 1), strides=(1, 1), padding='same', activation=tf.nn.relu6)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#cbam\n",
    "\n",
    "#cbam=CBAMBlock()(x)\n",
    "\n",
    "x = DepthwiseConv2D((3, 3), strides=(1, 1), padding='same', activation='relu')(x) #conv dw/s2\n",
    "x = Conv2D(256, (1, 1), strides=(1, 1), padding='same', activation=tf.nn.relu6)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = DepthwiseConv2D((3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "x = Conv2D(256, (1, 1), strides=(1, 1), padding='same', activation=tf.nn.relu6)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "cbam=CBAMBlock(name='cbamblock_1')(x)\n",
    "\n",
    "x = DepthwiseConv2D((3, 3), strides=(1, 1), padding='same', activation='relu')(cbam)\n",
    "x = Conv2D(512, (1, 1), strides=(1, 1), padding='same', activation=tf.nn.relu6)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = DepthwiseConv2D((3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = Conv2D(512, (1, 1), strides=(1, 1), padding='same', activation=tf.nn.relu6)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = DepthwiseConv2D((3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = Conv2D(512, (1, 1), strides=(1, 1), padding='same', activation=tf.nn.relu6)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "\n",
    "x = DepthwiseConv2D((3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = Conv2D(512, (1, 1), strides=(1, 1), padding='same', activation=tf.nn.relu6)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = DepthwiseConv2D((3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = Conv2D(512, (1, 1), strides=(1, 1), padding='same', activation=tf.nn.relu6)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#cbam=CBAMBlock(name='cbamblock_2')(x)\n",
    "\n",
    "x = DepthwiseConv2D((3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "x = Conv2D(1024, (1, 1), strides=(1, 1), padding='same', activation=tf.nn.relu6)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "x = DepthwiseConv2D((3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = Conv2D(1024, (1, 1), strides=(1, 1), padding='same', activation=tf.nn.relu6)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#cbam=CBAMBlock(name='cbamblock_3')(x)\n",
    "\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "\n",
    "x=Dense(1024, activation='relu')(x)\n",
    "\n",
    "out= Dense(7, activation='softmax')(x)\n",
    "\n",
    "# Create the Patch Extraction Block model\n",
    "model = Model(input_layer, out)\n",
    "\n",
    "\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T04:58:22.564317Z",
     "iopub.status.busy": "2024-06-09T04:58:22.563958Z",
     "iopub.status.idle": "2024-06-09T04:58:40.267798Z",
     "shell.execute_reply": "2024-06-09T04:58:40.266964Z",
     "shell.execute_reply.started": "2024-06-09T04:58:22.564287Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 1432 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path='/kaggle/input/newfer/train/'\n",
    "test_path='/kaggle/input/newfer/test/'\n",
    "train_generator,train_lbl,validation_generator,lbl= get_data(batch_size=64, target_size=(224,224), train_path=train_path,test_path=test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T04:59:48.311218Z",
     "iopub.status.busy": "2024-06-09T04:59:48.310477Z",
     "iopub.status.idle": "2024-06-09T04:59:48.321121Z",
     "shell.execute_reply": "2024-06-09T04:59:48.320320Z",
     "shell.execute_reply.started": "2024-06-09T04:59:48.311185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# adding class weights\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the class weights\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(lbl), y=lbl)\n",
    "# Convert the class weights to a dictionary\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-19T05:21:36.153302Z",
     "iopub.status.busy": "2023-09-19T05:21:36.152834Z",
     "iopub.status.idle": "2023-09-19T05:21:36.158978Z",
     "shell.execute_reply": "2023-09-19T05:21:36.157945Z",
     "shell.execute_reply.started": "2023-09-19T05:21:36.153262Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.8969202898550724, 1: 1.209035409035409, 2: 0.6866851595006935, 3: 1.5970967741935485, 4: 1.0250517598343685}\n"
     ]
    }
   ],
   "source": [
    "print(class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T05:11:24.796808Z",
     "iopub.status.busy": "2024-06-09T05:11:24.796352Z",
     "iopub.status.idle": "2024-06-09T05:11:24.810147Z",
     "shell.execute_reply": "2024-06-09T05:11:24.809216Z",
     "shell.execute_reply.started": "2024-06-09T05:11:24.796776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from math import pi\n",
    "import keras\n",
    "from tensorflow.keras.losses import Loss, CategoricalCrossentropy\n",
    "from tensorflow.keras import initializers\n",
    "import keras.backend as K\n",
    "\n",
    "class CombinedLoss(Loss):\n",
    "    def __init__(self, lambda_center=0.01, lambda_=0.001, margin=0.35, scale=64,num_classes=7,feature_dim=1024):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.lambda_center = lambda_center\n",
    "        self.lambda_ = lambda_\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        self.threshold = tf.math.cos(pi - margin)\n",
    "        self.cos_m = tf.math.cos(margin)\n",
    "        self.sin_m = tf.math.sin(margin)\n",
    "        self.safe_margin = self.sin_m * margin\n",
    "        self.feature_dim = feature_dim  # Update feature_dim to 1024\n",
    "        self.centers = K.variable(initializers.glorot_uniform()((feature_dim,num_classes)))\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, y_true, y_pred):\n",
    "        # CenterLoss\n",
    "        softmax_loss = CategoricalCrossentropy()(y_true, y_pred)\n",
    "        centers_batch = tf.gather(self.centers, tf.argmax(y_true, axis=1))\n",
    "        center_loss = softmax_loss + 0.01 * (0.5 * self.lambda_center * tf.reduce_sum(tf.square(y_pred - centers_batch)))\n",
    "\n",
    "        # ArcLoss\n",
    "        cos_t = y_pred\n",
    "        sin_t = tf.math.sqrt(1 - tf.math.square(cos_t))\n",
    "        cos_t_margin = tf.where(cos_t > self.threshold,\n",
    "                                cos_t * self.cos_m - sin_t * self.sin_m,\n",
    "                                cos_t - self.safe_margin)\n",
    "        mask = y_true\n",
    "        cos_t_onehot = cos_t * mask\n",
    "        cos_t_margin_onehot = cos_t_margin * mask\n",
    "        logits = (cos_t + cos_t_margin_onehot - cos_t_onehot) * self.scale\n",
    "        arc_loss = tf.nn.softmax_cross_entropy_with_logits(y_true, logits)\n",
    "\n",
    "        # Combined Loss\n",
    "        loss = self.lambda_ * center_loss + self.lambda_ * arc_loss\n",
    "        return loss\n",
    "\n",
    "    \n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CombinedLoss, self).get_config()\n",
    "        config.pop('reduction', None)\n",
    "        config.pop('name', None)\n",
    "        config.update({\"lambda_center\": self.lambda_center, \n",
    "                       \"lambda_\": self.lambda_, \n",
    "                       \"margin\": self.margin,\n",
    "                       \"scale\": self.scale\n",
    "                      })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T05:11:55.564432Z",
     "iopub.status.busy": "2024-06-09T05:11:55.563723Z",
     "iopub.status.idle": "2024-06-09T05:11:55.595798Z",
     "shell.execute_reply": "2024-06-09T05:11:55.594970Z",
     "shell.execute_reply.started": "2024-06-09T05:11:55.564398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "batch_size = 128\n",
    "weight_decay = 0.0001\n",
    "\n",
    "opt = SGD(learning_rate=initial_learning_rate,weight_decay=weight_decay,momentum=0.9)\n",
    "lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=10, monitor='val_accuracy', mode='max')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer = opt , loss=CombinedLoss(), metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T05:12:00.047973Z",
     "iopub.status.busy": "2024-06-09T05:12:00.047623Z",
     "iopub.status.idle": "2024-06-09T14:41:00.482622Z",
     "shell.execute_reply": "2024-06-09T14:41:00.481280Z",
     "shell.execute_reply.started": "2024-06-09T05:12:00.047944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "epochs = 100  # will increase late it was 300 n the code\n",
    "history = model.fit(train_generator,\n",
    "                         epochs=epochs,\n",
    "                         #steps_per_epoch=620,\n",
    "                         validation_data=validation_generator,\n",
    "                         class_weight=class_weights_dict,\n",
    "                         callbacks=[lr_scheduler])\n",
    "\n",
    "feature_extractor = Model(inputs=model.input, outputs=model.layers[-2].output)  # Output before the final dense layer\n",
    "train_features = feature_extractor.predict(train_generator, verbose=1)\n",
    "val_features = feature_extractor.predict(validation_generator, verbose=1)\n",
    "\n",
    "# Assuming train_lbl and lbl are in the correct format for RandomForestClassifier\n",
    "# Train the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(train_features, train_lbl)\n",
    "\n",
    "# Predict and evaluate the model using validation data\n",
    "val_predictions = rf_classifier.predict(val_features)\n",
    "val_accuracy = accuracy_score(lbl, val_predictions)\n",
    "print(f\"Validation accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Visualize the model performance\n",
    "plot_graph(history=history, model_name='CNN Model')\n",
    "plot_confusion_matrix(classes=classes, model=rf_classifier, model_name='Random Forest')\n",
    "\n",
    "#visualizing the model \n",
    "plot_graph(history=history, model_name='Model')\n",
    "#confusion matrix of the model\n",
    "plot_confusion_matrix(classes=classes, model=model, model_name='Model')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5176472,
     "sourceId": 8643214,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30554,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
